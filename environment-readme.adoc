= VDI Project Environment Variables
:icons: font
:source-highlighter: highlightjs
:gh-org: VEuPathDB
:gh-project: vdi-component-common
:gh-root-url: https://github.com/
:gh-org-url: {gh-root-url}/{gh-group}/
:gh-project-url: {gh-org-url}/{gh-project}

The following environment variables are declared by the
link:{gh-project-url}[{gh-project}] project.

== Static Environment Variables

[%header, cols="1,4m,2m,5"]
|===
| Req. | Key | Type | Description

4+^h| LDAP

| :sparkle:
| LDAP_SERVER
| List<HostAddress>
| List of host addresses for LDAP servers to connect to.

| :sparkle:
| ORACLE_BASE_DN
| String
| Base distinguished name used when querying for OracleNetDesc LDAP entries for
  VEuPathDB Oracle databases.

4+^h| Handler Configurations

|
| IMPORT_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of worker coroutines to use when processing import jobs.

|
| UPDATE_META_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of worker coroutines to use when processing metadata install/update
  jobs.

|
| INSTALL_DATA_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of worker coroutines to use when processing data install jobs.

|
| SHARE_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of worker coroutines to use when processing share update jobs.

|
| SOFT_DELETE_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of worker coroutines to use when processing soft delete jobs.

|
| HARD_DELETE_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of worker coroutines to use when processing hard delete jobs.

4+^h| Cache DB Configuration

| :sparkle:
| CACHE_DB_HOST
| String
| Hostname of the cache DB instance.

|
| CACHE_DB_PORT
| uint16
| Port for the cache DB connection.

| :sparkle:
| CACHE_DB_NAME
| String
| Name of the cache database.

| :sparkle:
| CACHE_DB_USERNAME
| String
| Username for the cache DB.

| :sparkle:
| CACHE_DB_PASSWORD
| String
| Password for the cache DB.

|
| CACHE_DB_POOL_SIZE
| uint8
| Max connection pool size for the cache database connections.

4+^h| Kafka General

| :sparkle:
| KAFKA_SERVERS
| List<HostAddress>
| List of Kafka servers that will be connected to for sending internal messages.

4+^h| Kafka Consumer Configuration

// region Kafka Consumer Config

|
| KAFKA_CONSUMER_AUTO_COMMIT_INTERVAL
| Duration
| Time interval for automatically committing the position of the last read
  message from the Kafka log.

|
| KAFKA_CONSUMER_AUTO_OFFSET_RESET
| Enum: +
  - earliest +
  - latest +
  - none
| What the default position in the message log should be when first subscribing
  to a Kafka topic.

| :sparkle:
| KAFKA_CONSUMER_CLIENT_ID
| String
| ID of the consumer.  This value is used to track which consumers have seen
  what log messages.

|
| KAFKA_CONSUMER_CONNECTIONS_MAX_IDLE
| Duration
| Length of time an idle consumer connection should remain open.

|
| KAFKA_CONSUMER_DEFAULT_API_TIMEOUT
| Duration
| ???

|
| KAFKA_CONSUMER_ENABLE_AUTO_COMMIT
| Boolean
| Enable or disable auto committing message log last read position.

|
| KAFKA_CONSUMER_FETCH_MAX_BYTES
| uint32
| Max number of bytes to fetch from Kafka at a time per API request.

|
| KAFKA_CONSUMER_FETCH_MIN_BYTES
| uint32
| Minimum number of bytes to wait for when fetching data from Kafka.

| :sparkle:
| KAFKA_CONSUMER_GROUP_ID
| String
| Group ID for the client.  Message log positions are tracked per group ID and
  distributed among the connected clients.

|
| KAFKA_CONSUMER_GROUP_INSTANCE_ID
| String
| ???

|
| KAFKA_CONSUMER_HEARTBEAT_INTERVAL
| Duration
| Heartbeat interval for keeping client connections alive.

|
| KAFKA_CONSUMER_MAX_POLL_INTERVAL
| Duration
| ???

|
| KAFKA_CONSUMER_MAX_POLL_RECORDS
| uint32
| Max number of records to return from Kafka in a single request.

|
| KAFKA_CONSUMER_POLL_DURATION
| Duration
| ???

|
| KAFKA_CONSUMER_RECEIVE_BUFFER_SIZE_BYTES
| uint32
| ???

|
| KAFKA_CONSUMER_RECONNECT_BACKOFF_MAX_TIME
| Duration
| Max backoff time for reconnecting to the Kafka server when the connection is
  lost.

|
| KAFKA_CONSUMER_RECONNECT_BACKOFF_TIME
| Duration
| Amount of time to backoff after a failed reconnection attempt before
  attempting to reconnect again.

|
| KAFKA_CONSUMER_REQUEST_TIMEOUT
| Duration
| ???

|
| KAFKA_CONSUMER_RETRY_BACKOFF_TIME
| Duration
| ???

|
| KAFKA_CONSUMER_SEND_BUFFER_SIZE_BYTES
| uint32
| ???

|
| KAFKA_CONSUMER_SESSION_TIMEOUT
| Duration
| ???

// endregion Kafka Consumer Config

4+^h| Kafka Producer Configuration

// region Kafka Producer Config

|
| KAFKA_PRODUCER_BATCH_SIZE
| uint32
| ???

|
| KAFKA_PRODUCER_BUFFER_MEMORY_BYTES
| uint32
| ???

| :sparkle:
| KAFKA_PRODUCER_CLIENT_ID
| String
| ???

|
| KAFKA_PRODUCER_COMPRESSION_TYPE
| Enum: +
  - none +
  - gzip +
  - snappy +
  - lz4 +
  - zstd
| ???

|
| KAFKA_PRODUCER_CONNECTIONS_MAX_IDLE
| Duration
| ???

|
| KAFKA_PRODUCER_DELIVERY_TIMEOUT
| Duration
| ???

|
| KAFKA_PRODUCER_LINGER_TIME
| Duration
| How long the Kafka producer should let messages "linger" to batch them
  together before sending off a batch of collected messages.

|
| KAFKA_PRODUCER_MAX_BLOCKING_TIMEOUT
| Duration
| ???

|
| KAFKA_PRODUCER_MAX_REQUEST_SIZE_BYTES
| uint32
| ???

|
| KAFKA_PRODUCER_RECEIVE_BUFFER_SIZE_BYTES
| uint32
| ???

|
| KAFKA_PRODUCER_RECONNECT_BACKOFF_MAX_TIME
| Duration
| ???

|
| KAFKA_PRODUCER_RECONNECT_BACKOFF_TIME
| Duration
| ???

|
| KAFKA_PRODUCER_REQUEST_TIMEOUT
| Duration
| ???

|
| KAFKA_PRODUCER_RETRY_BACKOFF_TIME
| Duration
| ???

|
| KAFKA_PRODUCER_SEND_BUFFER_SIZE_BYTES
| uint32
| ???

|
| KAFKA_PRODUCER_SEND_RETRIES
| uint32
| ???

// endregion Kafka Producer Config

4+^h| Kafka Topics

|
| KAFKA_TOPIC_HARD_DELETE_TRIGGERS
| String
| Name of the Kafka topic to which hard delete trigger messages should be
  delivered.

|
| KAFKA_TOPIC_IMPORT_TRIGGERS
| String
| Name of the Kafka topic to which import trigger messages should be delivered.

|
| KAFKA_TOPIC_INSTALL_TRIGGERS
| String
| Name of the Kafka topic to which install trigger messages should be delivered.

|
| KAFKA_TOPIC_SHARE_TRIGGERS
| String
| Name of the Kafka topic to which share trigger messages should be delivered.

|
| KAFKA_TOPIC_SOFT_DELETE_TRIGGERS
| String
| Name of the Kafka topic to which soft delete trigger messages should be
  delivered.

|
| KAFKA_TOPIC_UPDATE_META_TRIGGERS
| String
| Name of the Kafka topic to which update/install meta trigger messages should
  be delivered.

4+^h| Kafka Message Keys

|
| KAFKA_MESSAGE_KEY_HARD_DELETE_TRIGGERS
| String
| Message key for hard delete trigger messages delivered to Kafka.

|
| KAFKA_MESSAGE_KEY_IMPORT_TRIGGERS
| String
| Message key for import trigger messages delivered to Kafka.

|
| KAFKA_MESSAGE_KEY_INSTALL_TRIGGERS
| String
| Message key for install trigger messages delivered to Kafka.

|
| KAFKA_MESSAGE_KEY_SHARE_TRIGGERS
| String
| Message key for share trigger messages delivered to Kafka.

|
| KAFKA_MESSAGE_KEY_SOFT_DELETE_TRIGGERS
| String
| Message key for soft delete trigger messages delivered to Kafka.

|
| KAFKA_MESSAGE_KEY_UPDATE_META_TRIGGERS
| String
| Message key for update/install meta trigger messages delivered to Kafka.

|===

== Wildcard Environment Variables

Environment variables that contain a wildcard component.  These variables appear
in groups that may be repeated for each new wildcard value.  The wildcard value
will be represented in the tables below as `{NAME}`.

If a variable is marked as required, then it is required to appear in each group
defined by the wildcard name.

=== Plugin Handlers

Each group of environment variables that appears in the relevant application's
environment represents a plugin handler that will be registered to be used and
called on by the VDI service.

[%header, cols="1,4m,2m,5"]
|===
| Req. | Key | Type | Description

| :sparkle:
| PLUGIN_HANDLER_{NAME}_NAME
| String
| Plugin handler name.  This value MUST be a valid dataset type name.

| :sparkle:
| PLUGIN_HANDLER_{NAME}_ADDRESS
| HostAddress
| Host address of the plugin handler service to call
  for the target dataset type.

|
| PLUGIN_HANDLER_{NAME}_PROJECT_IDS
| List<String>
| Target projects for which this plugin handler is relevant.  If this value is
  blank or absent the VDI service will assume the plugin handler is relevant to
  all projects.

|===

=== Application Databases

[%header, cols="1,4m,2m,5"]
|===
| Req. | Key | Type | Description

| :sparkle:
| DB_CONNECTION_NAME_{NAME}
| String
| Name of the database connection, should be the name of a target application,
  such as a VEuPathDB project ID/name.

| :sparkle:
| DB_CONNECTION_LDAP_{NAME}
| String
| LDAP name of the entry containing the OracleNetDesc for the target Oracle database.

| :sparkle:
| DB_CONNECTION_USER_{NAME}
| String
| Username to use when connecting to the target database.

| :sparkle:
| DB_CONNECTION_PASS_{NAME}
| String
| Password to use when connecting to the target database.

| :sparkle:
| DB_CONNECTION_POOL_SIZE_{NAME}
| uint8
| Max size of the database connection pool for the target database.
|===

== Variable Types

`List<T>`::
A comma separated list of values of type `T`.
+
Example:
[source, shell]
----
FOO=happy,sad,mad
----

`Map<T>`::
A map of `String` keys to values of type `T`.  Map entries appear in the format
`{key}:{value}` separated by commas.  Map entry keys must not contain `:`
characters, however values may.
+
Type `T` must not be a `List` or `Map` value.
+
Example:
[source, shell]
----
FOO=mood1:happy,mood2:sad,mood3:mad
----

`Duration`::
???
+
Examples:
[source, shell]
----
FOO=1d
FOO=1h
FOO=1m
FOO=1s
FOO=1ms
FOO=1d 2h 3m 4.567s
----

`HostAddress`::
A hostname/port pairing in the format: `{hostname}:{port}`.
+
Example:
[source, shell]
----
FOO=google.com:80
----